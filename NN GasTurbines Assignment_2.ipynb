{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca73c32",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_2444/317173812.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\anku\\AppData\\Local\\Temp/ipykernel_2444/317173812.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    The dataset contains 36733 instances of 11 sensor measures aggregated over one hour (by means of average or sum)\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The dataset contains 36733 instances of 11 sensor measures aggregated over one hour (by means of average or sum)\n",
    "from a gas turbine. \n",
    "The Dataset includes gas turbine parameters (such as Turbine Inlet Temperature and Compressor Discharge pressure) \n",
    "in addition to the ambient variables.\n",
    "\n",
    "Problem statement: predicting turbine energy yield (TEY) using ambient variables as features.\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "The explanations of sensor measurements and their brief statistics are given below.\n",
    "\n",
    "Variable (Abbr.) Unit Min Max Mean\n",
    "Ambient temperature (AT) C â€“6.23 37.10 17.71\n",
    "Ambient pressure (AP) mbar 985.85 1036.56 1013.07\n",
    "Ambient humidity (AH) (%) 24.08 100.20 77.87\n",
    "Air filter difference pressure (AFDP) mbar 2.09 7.61 3.93\n",
    "Gas turbine exhaust pressure (GTEP) mbar 17.70 40.72 25.56\n",
    "Turbine inlet temperature (TIT) C 1000.85 1100.89 1081.43\n",
    "Turbine after temperature (TAT) C 511.04 550.61 546.16\n",
    "Compressor discharge pressure (CDP) mbar 9.85 15.16 12.06\n",
    "Turbine energy yield (TEY) MWH 100.02 179.50 133.51\n",
    "Carbon monoxide (CO) mg/m3 0.00 44.10 2.37\n",
    "Nitrogen oxides (NOx) mg/m3 25.90 119.91 65.29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e5889",
   "metadata": {},
   "source": [
    "\n",
    "* Loaded the dataset for Gas Turbines\n",
    "* Deleted gas turbine parameters (2 columns) in accordance with the problem statement.\n",
    "* Predicted TEY values using 2 approach:\n",
    "\n",
    "* 1. Standardized only predictor variables after train_test_split\n",
    "     Applied ANN model\n",
    "     Calculated best parameters for the batch size and no. of epochs\n",
    "     Trained the model with best parameters\n",
    "     Predicted TEY with an acccuracy of 99.60 %\n",
    "\n",
    "* 2. Standardized both predictor & response variables before train_test_split\n",
    "     Applied ANN model\n",
    "     Calculated best parameters for the batch size and no. of epochs\n",
    "     Trained the model with best parameters\n",
    "     Descaled the predictor & response variables\n",
    "     Predicted TEY with an acccuracy of 99.57 %\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d05462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4122cbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv(\"E:\\\\DATA SCIENCE\\\\LMS\\ASSIGNMENT\\\\MY ASSIGNMENT\\\\Neural Networks\\\\gas_turbines.csv\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2949f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TAT     TEY      CO     NOX\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  550.00  114.70  3.1547  82.722\n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  550.00  114.72  3.2363  82.776\n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  549.87  114.71  3.2012  82.468\n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  549.99  114.72  3.1923  82.670\n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  549.98  114.72  3.2484  82.311\n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...\n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  546.21  111.61  4.5186  79.559\n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  543.22  111.78  4.8470  79.917\n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  537.32  110.19  7.9632  90.912\n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  541.24  110.74  6.2494  93.227\n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  545.85  111.58  4.9816  92.498\n",
       "\n",
       "[15039 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.drop(['TIT','CDP'], axis=1, inplace=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17c0b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AT      float64\n",
      "AP      float64\n",
      "AH      float64\n",
      "AFDP    float64\n",
      "GTEP    float64\n",
      "TAT     float64\n",
      "TEY     float64\n",
      "CO      float64\n",
      "NOX     float64\n",
      "dtype: object\n",
      "AT      0\n",
      "AP      0\n",
      "AH      0\n",
      "AFDP    0\n",
      "GTEP    0\n",
      "TAT     0\n",
      "TEY     0\n",
      "CO      0\n",
      "NOX     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.00000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.764381</td>\n",
       "      <td>1013.19924</td>\n",
       "      <td>79.124174</td>\n",
       "      <td>4.200294</td>\n",
       "      <td>25.419061</td>\n",
       "      <td>545.396183</td>\n",
       "      <td>134.188464</td>\n",
       "      <td>1.972499</td>\n",
       "      <td>68.190934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.574323</td>\n",
       "      <td>6.41076</td>\n",
       "      <td>13.793439</td>\n",
       "      <td>0.760197</td>\n",
       "      <td>4.173916</td>\n",
       "      <td>7.866803</td>\n",
       "      <td>15.829717</td>\n",
       "      <td>2.222206</td>\n",
       "      <td>10.470586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.522300</td>\n",
       "      <td>985.85000</td>\n",
       "      <td>30.344000</td>\n",
       "      <td>2.087400</td>\n",
       "      <td>17.878000</td>\n",
       "      <td>512.450000</td>\n",
       "      <td>100.170000</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>27.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.408000</td>\n",
       "      <td>1008.90000</td>\n",
       "      <td>69.750000</td>\n",
       "      <td>3.723900</td>\n",
       "      <td>23.294000</td>\n",
       "      <td>542.170000</td>\n",
       "      <td>127.985000</td>\n",
       "      <td>0.858055</td>\n",
       "      <td>61.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.186000</td>\n",
       "      <td>1012.80000</td>\n",
       "      <td>82.266000</td>\n",
       "      <td>4.186200</td>\n",
       "      <td>25.082000</td>\n",
       "      <td>549.890000</td>\n",
       "      <td>133.780000</td>\n",
       "      <td>1.390200</td>\n",
       "      <td>66.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.862500</td>\n",
       "      <td>1016.90000</td>\n",
       "      <td>90.043500</td>\n",
       "      <td>4.550900</td>\n",
       "      <td>27.184000</td>\n",
       "      <td>550.060000</td>\n",
       "      <td>140.895000</td>\n",
       "      <td>2.160400</td>\n",
       "      <td>73.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.929000</td>\n",
       "      <td>1034.20000</td>\n",
       "      <td>100.200000</td>\n",
       "      <td>7.610600</td>\n",
       "      <td>37.402000</td>\n",
       "      <td>550.610000</td>\n",
       "      <td>174.610000</td>\n",
       "      <td>44.103000</td>\n",
       "      <td>119.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AT           AP            AH          AFDP          GTEP  \\\n",
       "count  15039.000000  15039.00000  15039.000000  15039.000000  15039.000000   \n",
       "mean      17.764381   1013.19924     79.124174      4.200294     25.419061   \n",
       "std        7.574323      6.41076     13.793439      0.760197      4.173916   \n",
       "min        0.522300    985.85000     30.344000      2.087400     17.878000   \n",
       "25%       11.408000   1008.90000     69.750000      3.723900     23.294000   \n",
       "50%       18.186000   1012.80000     82.266000      4.186200     25.082000   \n",
       "75%       23.862500   1016.90000     90.043500      4.550900     27.184000   \n",
       "max       34.929000   1034.20000    100.200000      7.610600     37.402000   \n",
       "\n",
       "                TAT           TEY            CO           NOX  \n",
       "count  15039.000000  15039.000000  15039.000000  15039.000000  \n",
       "mean     545.396183    134.188464      1.972499     68.190934  \n",
       "std        7.866803     15.829717      2.222206     10.470586  \n",
       "min      512.450000    100.170000      0.000388     27.765000  \n",
       "25%      542.170000    127.985000      0.858055     61.303500  \n",
       "50%      549.890000    133.780000      1.390200     66.601000  \n",
       "75%      550.060000    140.895000      2.160400     73.935500  \n",
       "max      550.610000    174.610000     44.103000    119.890000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds.dtypes)\n",
    "print(ds.isnull().sum())\n",
    "ds.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ddab2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.412953</td>\n",
       "      <td>-0.549432</td>\n",
       "      <td>-0.099333</td>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.207495</td>\n",
       "      <td>-0.088588</td>\n",
       "      <td>-0.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>-0.412953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.256744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AH</th>\n",
       "      <td>-0.549432</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>0.143061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFDP</th>\n",
       "      <td>-0.099333</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.037299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEP</th>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>-0.208496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAT</th>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEY</th>\n",
       "      <td>-0.207495</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>-0.102631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CO</th>\n",
       "      <td>-0.088588</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.600006</td>\n",
       "      <td>0.256744</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>-0.037299</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-0.102631</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AT        AP        AH      AFDP      GTEP       TAT       TEY  \\\n",
       "AT    1.000000 -0.412953 -0.549432 -0.099333 -0.049103  0.338569 -0.207495   \n",
       "AP   -0.412953  1.000000  0.042573  0.040318  0.078575 -0.223479  0.146939   \n",
       "AH   -0.549432  0.042573  1.000000 -0.119249 -0.202784  0.010859 -0.110272   \n",
       "AFDP -0.099333  0.040318 -0.119249  1.000000  0.744251 -0.571541  0.717995   \n",
       "GTEP -0.049103  0.078575 -0.202784  0.744251  1.000000 -0.756884  0.977042   \n",
       "TAT   0.338569 -0.223479  0.010859 -0.571541 -0.756884  1.000000 -0.720356   \n",
       "TEY  -0.207495  0.146939 -0.110272  0.717995  0.977042 -0.720356  1.000000   \n",
       "CO   -0.088588  0.041614  0.165505 -0.334207 -0.508259  0.063404 -0.541751   \n",
       "NOX  -0.600006  0.256744  0.143061 -0.037299 -0.208496  0.009888 -0.102631   \n",
       "\n",
       "            CO       NOX  \n",
       "AT   -0.088588 -0.600006  \n",
       "AP    0.041614  0.256744  \n",
       "AH    0.165505  0.143061  \n",
       "AFDP -0.334207 -0.037299  \n",
       "GTEP -0.508259 -0.208496  \n",
       "TAT   0.063404  0.009888  \n",
       "TEY  -0.541751 -0.102631  \n",
       "CO    1.000000  0.316743  \n",
       "NOX   0.316743  1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "347450dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TEY      AT      AP      AH    AFDP    GTEP     TAT      CO     NOX\n",
      "0  114.70  6.8594  1007.9  96.799  3.5000  19.663  550.00  3.1547  82.722\n",
      "1  114.72  6.7850  1008.4  97.118  3.4998  19.728  550.00  3.2363  82.776\n",
      "2  114.71  6.8977  1008.8  95.939  3.4824  19.779  549.87  3.2012  82.468\n",
      "3  114.72  7.0569  1009.2  95.249  3.4805  19.792  549.99  3.1923  82.670\n",
      "4  114.72  7.3978  1009.7  95.150  3.4976  19.765  549.98  3.2484  82.311\n"
     ]
    }
   ],
   "source": [
    "# moving the TEY column to the 0th position in the table\n",
    "lastCol = ds.pop('TEY')\n",
    "ds.insert(0 , 'TEY', lastCol)\n",
    "print(ds.head(5))\n",
    "\n",
    "#assigning predictor variables to x and response variable to y\n",
    "x = ds.iloc[:,1:]\n",
    "y = ds[['TEY']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b3093",
   "metadata": {},
   "source": [
    "### Standardizing only predictor variable - after train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9806ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12031, 8)\n",
      "(3008, 8)\n",
      "(12031, 1)\n",
      "(3008, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20, random_state=42)\n",
    "\n",
    "scaler_train = StandardScaler()\n",
    "scaler_test = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler_train.fit_transform(x_train) # scaling train data -- predictor\n",
    "x_test_scaled  = scaler_test.fit_transform(x_test) # scaling test data -- predictor\n",
    "\n",
    "print(x_train_scaled.shape)\n",
    "print(x_test_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "#for removing heading from y_test\n",
    "y_test = y_test.values\n",
    "#print(x_train_scaled)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd3d5080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 17289.2871 - mean_squared_error: 17289.2871\n",
      "Epoch 2/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 15618.7275 - mean_squared_error: 15618.7275\n",
      "Epoch 3/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 14339.5312 - mean_squared_error: 14339.5312\n",
      "Epoch 4/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 13177.0869 - mean_squared_error: 13177.0869\n",
      "Epoch 5/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 12098.4746 - mean_squared_error: 12098.4746\n",
      "Epoch 6/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 11091.1250 - mean_squared_error: 11091.1250\n",
      "Epoch 7/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 10147.8457 - mean_squared_error: 10147.8457\n",
      "Epoch 8/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 9263.6885 - mean_squared_error: 9263.6885\n",
      "Epoch 9/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 8434.9414 - mean_squared_error: 8434.9414\n",
      "Epoch 10/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 7658.5767 - mean_squared_error: 7658.5767\n",
      "Epoch 11/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 6932.4380 - mean_squared_error: 6932.4380\n",
      "Epoch 12/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 6254.0845 - mean_squared_error: 6254.0845\n",
      "Epoch 13/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 5621.4766 - mean_squared_error: 5621.4766\n",
      "Epoch 14/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 5032.9863 - mean_squared_error: 5032.9858\n",
      "Epoch 15/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 4487.1206 - mean_squared_error: 4487.1206\n",
      "Epoch 16/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 3982.3423 - mean_squared_error: 3982.3423\n",
      "Epoch 17/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 3517.2827 - mean_squared_error: 3517.2827\n",
      "Epoch 18/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 3090.5774 - mean_squared_error: 3090.5774\n",
      "Epoch 19/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 2700.8896 - mean_squared_error: 2700.8896\n",
      "Epoch 20/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 2346.9822 - mean_squared_error: 2346.9822\n",
      "Epoch 21/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 2027.5868 - mean_squared_error: 2027.5868\n",
      "Epoch 22/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1741.4333 - mean_squared_error: 1741.4333\n",
      "Epoch 23/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1486.9576 - mean_squared_error: 1486.9576\n",
      "Epoch 24/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1262.7876 - mean_squared_error: 1262.7877\n",
      "Epoch 25/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1067.2892 - mean_squared_error: 1067.2892\n",
      "Epoch 26/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 898.8904 - mean_squared_error: 898.8904\n",
      "Epoch 27/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 755.9058 - mean_squared_error: 755.9058\n",
      "Epoch 28/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 636.2421 - mean_squared_error: 636.2421\n",
      "Epoch 29/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 537.8804 - mean_squared_error: 537.8804\n",
      "Epoch 30/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 458.8449 - mean_squared_error: 458.8449\n",
      "Epoch 31/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 396.8668 - mean_squared_error: 396.8668\n",
      "Epoch 32/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 349.6898 - mean_squared_error: 349.6898\n",
      "Epoch 33/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 315.0441 - mean_squared_error: 315.0441\n",
      "Epoch 34/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 290.5347 - mean_squared_error: 290.5347\n",
      "Epoch 35/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 274.0094 - mean_squared_error: 274.0094\n",
      "Epoch 36/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 263.4936 - mean_squared_error: 263.4936\n",
      "Epoch 37/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 257.1979 - mean_squared_error: 257.1979\n",
      "Epoch 38/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 253.7127 - mean_squared_error: 253.7127\n",
      "Epoch 39/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 251.9601 - mean_squared_error: 251.9601\n",
      "Epoch 40/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 251.1397 - mean_squared_error: 251.1397\n",
      "Epoch 41/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 250.7929 - mean_squared_error: 250.7929\n",
      "Epoch 42/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 176.3850 - mean_squared_error: 176.3850\n",
      "Epoch 43/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 100.3726 - mean_squared_error: 100.3726\n",
      "Epoch 44/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 74.4401 - mean_squared_error: 74.4401\n",
      "Epoch 45/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 55.2820 - mean_squared_error: 55.2820\n",
      "Epoch 46/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 41.0393 - mean_squared_error: 41.0393\n",
      "Epoch 47/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 30.4892 - mean_squared_error: 30.4892\n",
      "Epoch 48/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 22.6983 - mean_squared_error: 22.6983\n",
      "Epoch 49/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 16.9982 - mean_squared_error: 16.9982\n",
      "Epoch 50/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 12.7909 - mean_squared_error: 12.7909\n",
      "Epoch 51/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 9.6748 - mean_squared_error: 9.6748\n",
      "Epoch 52/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 7.3431 - mean_squared_error: 7.3430\n",
      "Epoch 53/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 5.6339 - mean_squared_error: 5.6339\n",
      "Epoch 54/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 4.3726 - mean_squared_error: 4.3726\n",
      "Epoch 55/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 3.4415 - mean_squared_error: 3.4415\n",
      "Epoch 56/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 2.7725 - mean_squared_error: 2.7725\n",
      "Epoch 57/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 2.2991 - mean_squared_error: 2.2991\n",
      "Epoch 58/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1.9560 - mean_squared_error: 1.9560\n",
      "Epoch 59/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1.6758 - mean_squared_error: 1.6758\n",
      "Epoch 60/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1.4758 - mean_squared_error: 1.4758\n",
      "Epoch 61/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1.3351 - mean_squared_error: 1.3351\n",
      "Epoch 62/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1.2035 - mean_squared_error: 1.2035\n",
      "Epoch 63/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1.1156 - mean_squared_error: 1.1156\n",
      "Epoch 64/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 1.0349 - mean_squared_error: 1.0349\n",
      "Epoch 65/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.9726 - mean_squared_error: 0.9726\n",
      "Epoch 66/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.9234 - mean_squared_error: 0.9234\n",
      "Epoch 67/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.8701 - mean_squared_error: 0.8701\n",
      "Epoch 68/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.8385 - mean_squared_error: 0.8385\n",
      "Epoch 69/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.8162 - mean_squared_error: 0.8162\n",
      "Epoch 70/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.7887 - mean_squared_error: 0.7887\n",
      "Epoch 71/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.7532 - mean_squared_error: 0.7532\n",
      "Epoch 72/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.7153 - mean_squared_error: 0.7153\n",
      "Epoch 73/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.7019 - mean_squared_error: 0.7019\n",
      "Epoch 74/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6958 - mean_squared_error: 0.6958\n",
      "Epoch 75/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6917 - mean_squared_error: 0.6917\n",
      "Epoch 76/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6677 - mean_squared_error: 0.6677\n",
      "Epoch 77/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6503 - mean_squared_error: 0.6503\n",
      "Epoch 78/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6446 - mean_squared_error: 0.6446\n",
      "Epoch 79/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6369 - mean_squared_error: 0.6369\n",
      "Epoch 80/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6371 - mean_squared_error: 0.6371\n",
      "Epoch 81/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6274 - mean_squared_error: 0.6274\n",
      "Epoch 82/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6124 - mean_squared_error: 0.6124\n",
      "Epoch 83/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5973 - mean_squared_error: 0.5973\n",
      "Epoch 84/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6050 - mean_squared_error: 0.6050\n",
      "Epoch 85/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.6051 - mean_squared_error: 0.6051\n",
      "Epoch 86/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5833 - mean_squared_error: 0.5833\n",
      "Epoch 87/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5894 - mean_squared_error: 0.5894\n",
      "Epoch 88/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5967 - mean_squared_error: 0.5967\n",
      "Epoch 89/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5788 - mean_squared_error: 0.5788\n",
      "Epoch 90/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5690 - mean_squared_error: 0.5690\n",
      "Epoch 91/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5642 - mean_squared_error: 0.5642\n",
      "Epoch 92/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5534 - mean_squared_error: 0.5534\n",
      "Epoch 93/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5781 - mean_squared_error: 0.5781\n",
      "Epoch 94/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5523 - mean_squared_error: 0.5523\n",
      "Epoch 95/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5626 - mean_squared_error: 0.5626\n",
      "Epoch 96/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5470 - mean_squared_error: 0.5470\n",
      "Epoch 97/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5413 - mean_squared_error: 0.5413\n",
      "Epoch 98/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5492 - mean_squared_error: 0.5492\n",
      "Epoch 99/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5380 - mean_squared_error: 0.5380\n",
      "Epoch 100/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.5417 - mean_squared_error: 0.5417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25f9bf95580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we have continuous ouput, AF is not required in the o/p layer\n",
    "model = Sequential()\n",
    "model.add( Dense( units = 50 , activation = 'relu' , kernel_initializer = 'normal', input_dim = 8)) # input layer\n",
    "model.add( Dense( units = 20 , activation = 'tanh' , kernel_initializer = 'normal' )) # hidden layer\n",
    "model.add( Dense( units = 1  , kernel_initializer = 'normal' )) # o/p layer\n",
    "\n",
    "model.compile(optimizer ='adam', loss = 'mean_squared_error', metrics=['MeanSquaredError'])\n",
    "model.fit(x_train_scaled, y_train , batch_size=50, epochs=100,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 5 - epochs: 5 Accuracy: 98.61386024081793\n"
     ]
    }
   ],
   "source": [
    "def toFindBestParams(x_train_scaled, y_train, x_test_scaled, y_test):\n",
    "        \n",
    "    #defining list of hyperparameters\n",
    "    batch_size_list = [5 , 10 , 15 , 20]\n",
    "    epoch_list      = [5 , 10 , 50 , 100]\n",
    "     \n",
    "    bestParamTable = pd.DataFrame()\n",
    "    \n",
    "    for batch_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            \n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=50, input_dim=x_train_scaled.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "            \n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=20, kernel_initializer='normal', activation='tanh'))\n",
    " \n",
    "            # The output neuron is a single fully connected node \n",
    "            # Since we will be predicting a single number\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "            # Compiling the model\n",
    "            model.compile(optimizer ='adam', loss = 'mean_squared_error')\n",
    "            \n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(x_train_scaled, y_train , batch_size=batch_trial, epochs=epochs_trial,  verbose=0)\n",
    "                        \n",
    "            MAPE = np.mean(100 * (np.abs(y_test-model.predict(x_test_scaled))/y_test))  \n",
    "                        \n",
    "            bestParamTable=bestParamTable.append(pd.DataFrame(data=[[batch_trial, epochs_trial, 100-MAPE]],\n",
    "                                                        columns=['batchsize','epochs','Accuracy'] ))\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print('batch_size:', batch_trial,'-', 'epochs:',epochs_trial, 'Accuracy:',100-MAPE)\n",
    "\n",
    "    return bestParamTable\n",
    "\n",
    "# Calling the function\n",
    "finalParamTable_1 = toFindBestParams(x_train_scaled, y_train, x_test_scaled, y_test)\n",
    "finalParamTable_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f021d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting corresponding row values of the maximum value of 'Accuracy' column\n",
    "finalParamTable_1 = finalParamTable_1.reset_index()\n",
    "#print(finalParamTable_1)\n",
    "#print(finalParamTable['Accuracy'].idxmax())\n",
    "finalParamTable_1.iloc[finalParamTable_1['Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83543d",
   "metadata": {},
   "source": [
    "## Training Model - using best params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer ='adam', loss = 'mean_squared_error')\n",
    "# fitting the model to best params\n",
    "model.fit(x_train_scaled,y_train, batch_size=20 , epochs = 100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8444ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generating predictions for test data\n",
    "y_predict_test = model.predict(x_test_scaled) \n",
    "\n",
    "# creating table with test price & predicted price for test\n",
    "final_table = pd.DataFrame(x_test)\n",
    "final_table['Price'] = y_test\n",
    "final_table['Predicted Price'] = y_predict_test\n",
    "print(final_table.shape)\n",
    "final_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the absolute percent error\n",
    "APE=100*(abs(final_table['Price']-final_table['Predicted Price'])/final_table['Price'])\n",
    "print('The Accuracy for Test Data -- ANN model = ', 100-np.mean(APE))\n",
    "\n",
    "# adding absolute percent error to table\n",
    "final_table['APE']=APE\n",
    "final_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209ea64",
   "metadata": {},
   "source": [
    "### Standardizing both Predictor & Response variable  - before train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sandardization of data ###\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    " \n",
    "# Storing the fit object for later reference\n",
    "x_scaler_fit = scaler_x.fit(x)\n",
    "y_scaler_fit = scaler_y.fit(y)\n",
    " \n",
    "# Generating the standardized values of X and y\n",
    "x = x_scaler_fit.transform(x)\n",
    "y = y_scaler_fit.transform(y)\n",
    " \n",
    "# Split the data into training and testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    " \n",
    "# Shape of Training and Test datasets\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have continuous ouput, AF is not required in the o/p layer\n",
    "model = Sequential()\n",
    "model.add( Dense( units = 50 , activation = 'relu' , kernel_initializer = 'normal', input_dim = 8)) # input layer\n",
    "model.add( Dense( units = 20 , activation = 'tanh' , kernel_initializer = 'normal' )) # hidden layer\n",
    "model.add( Dense( units = 1  , kernel_initializer = 'normal' )) # o/p layer\n",
    "\n",
    "model.compile(optimizer ='adam', loss = 'mean_squared_error', metrics=['mae'])\n",
    "model.fit(x_train, y_train , batch_size=50, epochs=100,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toFindBestParams(x_train, y_train, x_test, y_test):\n",
    "        \n",
    "    #defining list of hyperparameters\n",
    "    batch_size_list = [5 , 10 , 15 , 20]\n",
    "    epoch_list      = [5 , 10 , 50 , 100]\n",
    "    \n",
    "    bestParamTable = pd.DataFrame()\n",
    "    \n",
    "    for batch_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "                        \n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=50, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "            \n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=20, kernel_initializer='normal', activation='tanh'))\n",
    " \n",
    "            # The output neuron is 1 as o/p is continuous\n",
    "            # No AF needed coz continuous output\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "            # Compiling the model\n",
    "            model.compile(optimizer ='adam', loss = 'mean_squared_error')\n",
    "            \n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(x_train, y_train , batch_size=batch_trial, epochs=epochs_trial,  verbose=0)\n",
    "                        \n",
    "            MAPE = np.mean(100 * (np.abs(y_test-model.predict(x_test))/y_test))\n",
    "            \n",
    "            bestParamTable=bestParamTable.append(pd.DataFrame(data=[[batch_trial, epochs_trial, 100-MAPE]],\n",
    "                                                        columns=['batchsize','epochs','Accuracy'] ))\n",
    "            \n",
    "            #printing the results of the current iteration\n",
    "            print('batch_size:', batch_trial,'-', 'epochs:',epochs_trial, 'Accuracy:',100-MAPE)\n",
    "            \n",
    "    return bestParamTable\n",
    "\n",
    "# Calling the function\n",
    "finalParamTable = toFindBestParams(x_train, y_train, x_test, y_test)\n",
    "#print(finalParamTable['Accuracy'].max())\n",
    "#print(finalParamTable[finalParamTable['Accuracy'].max()])\n",
    "finalParamTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting corresponding row values of the maximum value of 'Accuracy' column\n",
    "finalParamTable = finalParamTable.reset_index()\n",
    "#print(finalParamTable)\n",
    "#print(finalParamTable['Accuracy'].idxmax())\n",
    "finalParamTable.iloc[finalParamTable['Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632223fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model to best params\n",
    "model.compile(optimizer ='adam', loss = 'mean_squared_error')\n",
    "model.fit(x_train,y_train, batch_size=10 , epochs = 50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2aca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating predictions for test data\n",
    "y_predict_test = model.predict(x_test) \n",
    "\n",
    "# scaling back test data to original data\n",
    "y_test_original = y_scaler_fit.inverse_transform(y_test)\n",
    "\n",
    "# Scaling the predicted Price data back to original price scale\n",
    "y_predict_test=y_scaler_fit.inverse_transform(y_predict_test)\n",
    "\n",
    "# scaling the test input data back to original\n",
    "x_test_original = x_scaler_fit.inverse_transform(x_test)\n",
    "\n",
    "# creating table with descaled test price & descaled predicted price for test\n",
    "final_table_1 = pd.DataFrame(x_test_original)\n",
    "final_table_1['Price'] = y_test_original\n",
    "final_table_1['Predicted Price'] = y_predict_test\n",
    "print(final_table_1.shape)\n",
    "final_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the absolute percent error\n",
    "APE_1 = 100*(abs(final_table_1['Price']-final_table_1['Predicted Price'])/final_table_1['Price'])\n",
    "print('The Accuracy for Test Data -- ANN model = ', 100-np.mean(APE_1))\n",
    "\n",
    "# adding absolute percent error to table\n",
    "final_table_1['APE'] = APE_1\n",
    "final_table_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039dd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176fbb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b10d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3bd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
